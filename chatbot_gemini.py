import os
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

# --- CONFIG ---
FOLDER_PATH = "logi_chatbot.txt"  # File ch·ª©a d·ªØ li·ªáu
FAISS_INDEX_PATH = "faiss_index"  # N∆°i l∆∞u FAISS index
GOOGLE_API_KEY = "AIzaSyB5Rv1wk7qiEsf1P8NSMYXrAZVoo7D3_H0"
MODEL_NAME = "gemini-1.5-pro"  # S·ª≠ d·ª•ng model m·∫°nh h∆°n

# --- STEP 1: Load file .txt ---
def load_text_file(file_path):
    loader = TextLoader(file_path, encoding="utf-8")
    documents = loader.load()
    return documents

# --- STEP 2: T·∫°o FAISS Index ---
def create_faiss_index(documents):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(documents, embeddings)
    vectorstore.save_local(FAISS_INDEX_PATH)

# --- STEP 3: T√¨m ki·∫øm v·ªõi FAISS + H·ªèi Gemini ---
def query_with_gemini(user_query, k=5):
    # Load FAISS
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)

    # T√¨m ki·∫øm n·ªôi dung li√™n quan
    related_docs = vectorstore.similarity_search(user_query, k=k)
    context = "\n\n".join([doc.page_content for doc in related_docs])

    # Ki·ªÉm tra n·∫øu FAISS kh√¥ng t√¨m th·∫•y n·ªôi dung ph√π h·ª£p
    if not context.strip():
        return "‚ùå Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong d·ªØ li·ªáu!"

    # K·∫øt n·ªëi Gemini API
    llm = ChatGoogleGenerativeAI(
        model=MODEL_NAME,
        google_api_key=GOOGLE_API_KEY,
        temperature=0.3
    )

    # T·∫°o prompt g·ª≠i ƒë·∫øn Gemini
    full_prompt = f"""
    D·ª±a v√†o c√°c th√¥ng tin sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c:

    ---Th√¥ng tin---
    {context}

    ---C√¢u h·ªèi---
    {user_query}
    """
    # G·ª≠i ƒë·∫øn Gemini
    response = llm.invoke(full_prompt)
    return response.content if response else "‚ùå L·ªói: Gemini kh√¥ng ph·∫£n h·ªìi!"

# --- MAIN ---
if __name__ == "__main__":
    # B∆∞·ªõc 1: Load file v√† t·∫°o FAISS (ch·∫°y 1 l·∫ßn khi c√≥ d·ªØ li·ªáu m·ªõi)
    documents = load_text_file(FOLDER_PATH)
    create_faiss_index(documents)

    # B∆∞·ªõc 2: H·ªèi ƒë√°p qua FAISS + Gemini
    while True:
        query = input("\nNh·∫≠p c√¢u h·ªèi (ho·∫∑c 'exit' ƒë·ªÉ tho√°t): ")
        if query.lower() == "exit":
            break
        answer = query_with_gemini(query)
        print("\nü§ñ Chatbot tr·∫£ l·ªùi:", answer)
